{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup",
      "mount_file_id": "1ntn8_faNNHEqDRcYsVwg6hgMsakfbQ61",
      "authorship_tag": "ABX9TyMlZv1in5e42IR5/EF3XynF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meyush0/Text-Classification/blob/main/Text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSYJu5EljWKC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/augmented_dataset_1000.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "HoKh3q83jbpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "kHKlrbK0kdWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "2Mlj1UsWnD8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Feedback'][526]"
      ],
      "metadata": {
        "id": "TJwolzt1y-NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning the Feedback\n",
        "ps=PorterStemmer()\n",
        "corpus=[]\n",
        "for i in range(0,len(data)):\n",
        "    print(i)\n",
        "    hmm=re.sub('[^a-zA-Z]',\"  \",data['Feedback'][i])\n",
        "    hmm=hmm.lower()\n",
        "    hmm=hmm.split()\n",
        "\n",
        "\n",
        "    hmm = ' '.join([ps.stem(words) for words in hmm if not words in stopwords.words('english')])\n",
        "\n",
        "\n",
        "    corpus.append(hmm)"
      ],
      "metadata": {
        "id": "D6eHfTOI_VDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "id": "Uzbx8QT7AiLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[6555]"
      ],
      "metadata": {
        "id": "He5GkXZnA2V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot"
      ],
      "metadata": {
        "id": "MYFoUfZNrehL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One hot layer"
      ],
      "metadata": {
        "id": "U0J53r4MBvfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "voc_size=20000"
      ],
      "metadata": {
        "id": "MVRNY9kWxXvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot=[one_hot(words,voc_size) for words in corpus]\n",
        "one_hot\n"
      ],
      "metadata": {
        "id": "3QQW6uFoxsgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one_hot_implementation = [one_hot(' '.join(feedback), voc_size) for feedback in data['Feedback']]\n",
        "#print(one_hot_implementation)"
      ],
      "metadata": {
        "id": "1lXJnTBtBTCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_len=60\n",
        "embeded_sent=pad_sequences(one_hot,padding='pre',maxlen=sent_len)"
      ],
      "metadata": {
        "id": "VXkyVpsdr2dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeded_sent"
      ],
      "metadata": {
        "id": "lpJEvqlsy5Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeded_sent.size"
      ],
      "metadata": {
        "id": "_eR1sZAQxWlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Theme']"
      ],
      "metadata": {
        "id": "B7Yb1oPrPCfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Theme'].value_counts()"
      ],
      "metadata": {
        "id": "gBWvnnS7UES0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "pqPNkR3PtyQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = ['Theme']\n",
        "encoder = OneHotEncoder()\n",
        "encoded_features = encoder.fit_transform(data[categorical_columns])\n",
        "encoded_features_array = encoded_features.toarray()\n",
        "\n",
        "encoded_features_df=pd.DataFrame(encoded_features_array,columns=encoder.get_feature_names_out(categorical_columns))\n",
        "X=pd.concat([data, encoded_features_df],axis=1)"
      ],
      "metadata": {
        "id": "Nx3RsMklDyJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "AaEiqqlZORys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theme_mapping = {\n",
        "    'Assessment, Marking and Feedback': 0,\n",
        "    'Enrolment, Induction and Academic Support': 1,\n",
        "    'Teaching on my Course': 2,\n",
        "    'Organisation and Management': 3,\n",
        "    'Student Voice': 4,\n",
        "    'Other': 5,\n",
        "    'Learning Resources, Opportunities and Community': 6\n",
        "}"
      ],
      "metadata": {
        "id": "XSbDZFPXT-rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Numerical_Theme'] = data['Theme'].map(theme_mapping)\n",
        "print(data[['Theme', 'Numerical_Theme']])"
      ],
      "metadata": {
        "id": "VABbd8IlXeTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the data"
      ],
      "metadata": {
        "id": "oQNqEKNtZtGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y=data['Numerical_Theme']\n",
        "y"
      ],
      "metadata": {
        "id": "j9ObzNroYyMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=embeded_sent\n",
        "x"
      ],
      "metadata": {
        "id": "Rtizyhr7Xixz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training the model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "cw6bLUMkYBZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x\n",
        "                                                    , y\n",
        "                                                    , test_size=0.2,\n",
        "                                                    random_state=25)\n",
        "x_train_processed = [' '.join(map(str, array)) for array in x_train]\n",
        "x_test_processed = [' '.join(map(str, array)) for array in x_test]"
      ],
      "metadata": {
        "id": "XfdhMFXgZIKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "tUvTPf62bT7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NaiveBayes/"
      ],
      "metadata": {
        "id": "TXyTmYotk7oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = make_pipeline(\n",
        "    TfidfVectorizer(),\n",
        "    MultinomialNB()\n",
        ")\n",
        "model.fit(x_train_processed,y_train)"
      ],
      "metadata": {
        "id": "cWwTYgrwbXYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model.predict(x_test_processed)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "ZS4xHFPTfODJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
      ],
      "metadata": {
        "id": "qFbYVYLtbmM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 8), yticklabels=range(1, 8))\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ev2czrZ7fM2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "vTz08CItgnwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "jZkGqOfrg7Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'tfidfvectorizer__max_features': [5000, 10000, 20000],\n",
        "    'tfidfvectorizer__ngram_range': [(1, 1), (1, 2)],\n",
        "    'multinomialnb__alpha': [0.1, 0.5, 1.0]\n",
        "}"
      ],
      "metadata": {
        "id": "C5x9aixnhUhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(x_train_processed, y_train)"
      ],
      "metadata": {
        "id": "bt2DUcBphXRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Parameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "id": "u8sATqHzha9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Accuracy:\", grid_search.best_score_ * 100)"
      ],
      "metadata": {
        "id": "n-ziztl4i-x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Npv28CKxjCT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier"
      ],
      "metadata": {
        "id": "f06k6-qWlPif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = make_pipeline(\n",
        "     TfidfVectorizer(),\n",
        "     XGBClassifier()\n",
        ")"
      ],
      "metadata": {
        "id": "f5jwcMovlShO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.fit(x_train_processed,y_train)"
      ],
      "metadata": {
        "id": "ukH82PVEmwxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model2.predict(x_test_processed)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "0ZbBh701nUI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy = accuracy_score(y_test, y_pred)\n",
        "#prinblend_test_set = np.column_stack((pred_nb_test, pred_svm_test))(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "kgeXp-wppFQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_rf = {\n",
        "    'tfidfvectorizer__max_features': [5000, 10000, 20000],\n",
        "    'tfidfvectorizer__ngram_range': [(1, 1), (1, 2)],\n",
        "    'randomforestclassifier__n_estimators': [50, 100, 200],\n",
        "    'randomforestclassifier__max_depth': [None, 10, 20],\n",
        "    'randomforestclassifier__min_samples_split': [2, 5, 10]\n",
        "}"
      ],
      "metadata": {
        "id": "ct1oiCMZpJw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search_rf = GridSearchCV(model2, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)"
      ],
      "metadata": {
        "id": "_acb_nOLp6M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# voting classifier # we can use multiple algo in the pipeline\n",
        "#ensemble_model = make_pipeline(\n",
        "       # PreprocessingStep(),\n",
        "       # TfidfVectorizer(),\n",
        "       # FunctionTransformer(lambda x: x.toarray(), accept_sparse=True),\n",
        "# SomeAlgo())"
      ],
      "metadata": {
        "id": "m2OlHo9TqQF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "impleminting VotingClassifier"
      ],
      "metadata": {
        "id": "__IFJr8X5AyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier1 = RandomForestClassifier(random_state=42)\n",
        "classifier2 = GradientBoostingClassifier(random_state=42)\n",
        "classifier3 = LogisticRegression(random_state=42)"
      ],
      "metadata": {
        "id": "wP7p7N1B4n_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_hard = VotingClassifier(\n",
        "    estimators=[('rf',RandomForestClassifier(random_state=42)), ('gb', GradientBoostingClassifier(random_state=42)), ('lr', LogisticRegression(random_state=42))],\n",
        "    voting='hard')"
      ],
      "metadata": {
        "id": "IF7vcZru4jot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_soft = VotingClassifier(\n",
        "    estimators=[('rf',RandomForestClassifier(random_state=42)), ('gb', GradientBoostingClassifier(random_state=42)), ('lr', LogisticRegression(random_state=42))],\n",
        "    voting='soft')"
      ],
      "metadata": {
        "id": "PkGGJhse4jqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impleminting ANN"
      ],
      "metadata": {
        "id": "0-bOoDoJAeq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import ReLU,LeakyReLU,ELU,PReLU\n",
        "from tensorflow.keras.layers import Dropout"
      ],
      "metadata": {
        "id": "CJCjboQ25ZUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yep=tf.keras.callbacks.EarlyStopping(monitor='val_loss',min_delta=0.2,verbose=0,baseline=None,restore_best_weights=True,\n",
        "                                             start_from_epoch=0,patience=2)     #applying earlystopping"
      ],
      "metadata": {
        "id": "DUufgjStAm5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Initiate"
      ],
      "metadata": {
        "id": "qlMVhy6VBk6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Dense(units=10,activation='relu'))\n",
        "model.add(Dense(7, activation='softmax'))"
      ],
      "metadata": {
        "id": "4NKd_1-RBbPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt=tf.keras.optimizers.Adam(learning_rate=0.01)  #adding optimizer"
      ],
      "metadata": {
        "id": "rZnwZZEJBxfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=opt,loss='mean_squared_error',metrics=['mean_absolute_error'])    #complir the model"
      ],
      "metadata": {
        "id": "9CsBnMJrDFri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting the model\n",
        "ANN=model.fit(x_train, y_train,validation_split=0.2,batch_size=50,epochs=70)"
      ],
      "metadata": {
        "id": "VskdZ7qyDHDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "5DAjI8GHDbXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ANN.history.keys()"
      ],
      "metadata": {
        "id": "1rX_DB9tEHp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model.predict(x_test)"
      ],
      "metadata": {
        "id": "7pbe64UjEPvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "ophMalFTEWtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Blending Ensemble learning technique"
      ],
      "metadata": {
        "id": "60Lqy_w6a3i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "5piJDQYMEglJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the first base model (Multinomial Naive Bayes)\n",
        "model_nb = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "model_nb.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "MS258QljbSNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the second base model (Support Vector Machine)\n",
        "model_svm = make_pipeline(TfidfVectorizer(), SVC(probability=True))\n",
        "model_svm.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "W7iFfCr4bl8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_nb = model_nb.predict(x_blend)   #First base model prediction"
      ],
      "metadata": {
        "id": "R1fqSd3Yb0oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_svm = model_svm.predict(x_blend)  #predict from second base model"
      ],
      "metadata": {
        "id": "ytvdYF7vcNCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a blend set for the meta-model\n",
        "blend_set = np.column_stack((pred_nb, pred_svm))\n",
        "print(blend_set)"
      ],
      "metadata": {
        "id": "XHFGEd3zcZ8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the meta-model (Random Forest)\n",
        "meta_model = RandomForestClassifier()\n",
        "meta_model.fit(blend_set, y_blend)"
      ],
      "metadata": {
        "id": "VIRGHsducg6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_nb_test = model_nb.predict(x_test_processed)\n",
        "pred_svm_test = model_svm.predict(x_test_processed)"
      ],
      "metadata": {
        "id": "3X27OAKdc_KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blend_test_set = np.column_stack((pred_nb_test, pred_svm_test))\n",
        "final_predictions = meta_model.predict(blend_test_set)"
      ],
      "metadata": {
        "id": "k7ouOFubmLfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the final ensemble model\n",
        "ensemble_accuracy = accuracy_score(y_test, final_predictions)\n",
        "ensemble_accuracy*100"
      ],
      "metadata": {
        "id": "qrEENFOCoDmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplying more RNN model of Deep learning"
      ],
      "metadata": {
        "id": "CT_iFmpLwE7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense\n",
        "from sklearn.base import BaseEstimator, TransformerMixin   # framework for creating  custom model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "rC18l3HrpH66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMpipeline(BaseEstimator , TransformerMixin):\n",
        "  def __init__(self, voc_size=20000 , max_len=60 , lstm_units=64 , epochs=7 , batch_size=32 , dropout=0.2):\n",
        "    self.voc_size = voc_size\n",
        "    self.max_len = max_len\n",
        "    self.lstm_units = lstm_units\n",
        "    self.dropout = dropout\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.model = None\n",
        "    self.tokenizer = None\n",
        "\n",
        "  def fit(self, x, y):\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=self.voc_size, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    x_sequences = tokenizer.texts_to_sequences(x)\n",
        "    x_padded = pad_sequences(x_sequences, maxlen=self.max_len, padding='pre')\n",
        "\n",
        "    self.model = Sequential()\n",
        "    self.model.add(Embedding(self.voc_size, 128, input_length=self.max_len))\n",
        "    self.model.add(LSTM(self.lstm_units, dropout=self.dropout))\n",
        "    self.model.add(Dense(len(set(y)), activation='softmax'))\n",
        "\n",
        "    self.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    self.model.fit(x_padded, y, epochs=self.epochs, batch_size=self.batch_size)\n",
        "\n",
        "    return self\n",
        "\n",
        "  def transform(self, x):\n",
        "    x_sequences = tokenizer.texts_to_sequences(x)\n",
        "    X_padded = pad_sequences(x_sequences, maxlen=self.max_len, padding='pre')\n",
        "    return x_padded\n"
      ],
      "metadata": {
        "id": "2amXSJu7wzYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(self, x):\n",
        "        x_transformed = self.transform(x)\n",
        "        y_pred = self.model.predict_classes(x_transformed)\n",
        "        return y_pred"
      ],
      "metadata": {
        "id": "H1wjws4f0dTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_pipeline = Pipeline([\n",
        "    ('lstm', LSTMpipeline())\n",
        "])"
      ],
      "metadata": {
        "id": "-cLZNzyH9XIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_pipeline.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "HySZY6LL9f2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for augmented_data_set_1500"
      ],
      "metadata": {
        "id": "2ZhiEz5NM8os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = pd.read_csv('/content/augmented_dataset_1500.csv')\n",
        "data2.head()"
      ],
      "metadata": {
        "id": "CkPUKBInAFTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_split(data, text_column, label_column, voc_size=20000, sent_len=60, test_size=0.2, random_state=42):\n",
        "    # Clean the text data\n",
        "    corpus = []\n",
        "    for i in range(len(data)):\n",
        "        hmm = re.sub('[^a-zA-Z]', \" \", data[text_column][i])\n",
        "        hmm = hmm.lower()\n",
        "        hmm = hmm.split()\n",
        "        hmm = ' '.join([ps.stem(words) for words in hmm if not words in stopwords.words('english')])\n",
        "        corpus.append(hmm)\n",
        "\n",
        "    # One-hot encode the text data\n",
        "    one_hot_encoded_list = [one_hot(words, voc_size) for words in corpus]\n",
        "\n",
        "    # Pad sequences for uniform length\n",
        "    embedded_sentences = pad_sequences(one_hot_encoded_list, padding='pre', maxlen=sent_len)\n",
        "\n",
        "    # Encode the labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    encoded_labels = label_encoder.fit_transform(data[label_column])\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    x_train, x_test, y_train, y_test = train_test_split(embedded_sentences, encoded_labels, test_size=test_size, random_state=random_state)\n",
        "\n",
        "    return x_train, x_test, y_train, y_test\n"
      ],
      "metadata": {
        "id": "MArmNph9CpBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMpipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, voc_size=20000, max_len=60, lstm_units=64, epochs=7, batch_size=32, dropout=0.2):\n",
        "        self.voc_size = voc_size\n",
        "        self.max_len = max_len\n",
        "        self.lstm_units = lstm_units\n",
        "        self.dropout = dropout\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        self.tokenizer = Tokenizer(num_words=self.voc_size, oov_token=\"<OOV>\")\n",
        "        self.tokenizer.fit_on_texts(x)\n",
        "        x_sequences = self.tokenizer.texts_to_sequences(x)\n",
        "        x_padded = pad_sequences(x_sequences, maxlen=self.max_len, padding='pre')\n",
        "\n",
        "        num_classes = len(set(y))\n",
        "\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Embedding(self.voc_size, 128, input_length=self.max_len))\n",
        "        self.model.add(LSTM(self.lstm_units, dropout=self.dropout))\n",
        "        self.model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "        self.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        self.model.fit(x_padded, y, epochs=self.epochs, batch_size=self.batch_size)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, x):\n",
        "      x_sequences = self.tokenizer.texts_to_sequences(x)\n",
        "      x_padded = pad_sequences(x_sequences, maxlen=self.max_len, padding='pre')\n",
        "      return x_padded"
      ],
      "metadata": {
        "id": "if5ulBdETEK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_pipeline = Pipeline([\n",
        "    ('lstm', LSTMpipeline())\n",
        "])"
      ],
      "metadata": {
        "id": "Mzzdt8yPTVmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_pipeline.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "RQ6OaEMGVj3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5eGYsJEJVmJx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}